{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Sparse and dense representations for text data</font>\n",
    "\n",
    "Before we can start training we need to prepare our input data in a way that our model will understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're dealing with text, we need to turn the characters into numbers in order to perform our calculations on them. We do this in two steps: first we get the sparse (one-hot encoded) representation of each character and then we learn a dense representation (so-called embeddings) as part of our model training.\n",
    "\n",
    "#### Sparse representation: *one-hot encoding*\n",
    "Our sparse representation will consist of sparse vectors of dimension `n_chars`, which in our case is 129 (128 ascii chars + 1 end-of-sequence char). The feature vector for a single character will thus be of the form:\n",
    "\n",
    "$\\qquad x(\\text{char})\\ =\\ (0, 0, 1, 0, \\dots, 0)$\n",
    "\n",
    "Or equivalently in components,\n",
    "\n",
    "$\\qquad x_i(\\text{char})\\ =\\ \\left\\{\\begin{matrix}1&\\text{if } i = h(\\text{char})\\\\0&\\text{otherwise}\\end{matrix}\\right.$\n",
    "\n",
    "where $h$ is a function that maps a character to an integer (e.g. a hash function). In our case, we use the build-in function [**`ord`**](https://docs.python.org/2/library/functions.html#ord):\n",
    "```python\n",
    "In [1]: ord('H')\n",
    "Out[1]: 72\n",
    "```\n",
    "As it turns out, we don't actually need to construct the vector $x(\\text{char})$ as displayed above. If you think about it, the only information that we need about $x$ is which component is switched on. In other words, the only information we need is $h(\\text{char})$, in our case <font face='monospace'><font color='green'>ord</font>(char)</font>. So, the most efficient representation for our sparse feature vectors (single integers) turns out to be incredibly simple. For instance, the sparse representation of the phrase *\"Hello, world!\"* is simply:\n",
    "```python\n",
    "In [1]: x = [ord(char) for char in \"Hello, world!\"]\n",
    "In [2]: x\n",
    "Out[2]: [72, 101, 108, 108, 111, 44, 32, 119, 111, 114, 108, 100, 33]\n",
    "```\n",
    "Actually, we need to append an end-of-sequence (EOS) character to tell our model to stop generating more text. Let's set the index 0 aside for the EOS character, then we one-hot encode our phrase as follows:\n",
    "```python\n",
    "In [1]: x = [ord(char) + 1 for char in \"Hello, world!\"] + [0]\n",
    "In [2]: x\n",
    "Out[2]: [73, 102, 109, 109, 112, 45, 33, 120, 112, 115, 109, 101, 34, 0]\n",
    "```\n",
    "To go from a list of indices to a one-hot encoded vector in Tensorflow is super easy using **`tf.one_hot`**:\n",
    "```python\n",
    "n_chars = 129\n",
    "x_indices = tf.constant([73, 102, 109, 109, 112])\n",
    "x_one_hot = tf.one_hot(x_indices, n_chars)  # shape = (5, 129)\n",
    "\n",
    "```\n",
    "\n",
    "#### Dense representation: *embeddings*\n",
    "If we only have a few input characters, we can use the one-hot encoded representation directly as our input. In reality, though, we know that text consists of a large number characters (in our case 129). In this case it's either infeasible or at best highly inefficient to use the sparse representation for our characters.\n",
    "\n",
    "Moreover, the sparse representation has no notion of proximity between characters such as <font color='red' face='monospace'>'a'</font> and <font color='red' face='monospace'>'A'</font> or more subtly <font color='red' face='monospace'>'i'</font> and <font color='red' face='monospace'>'y'</font>.\n",
    "\n",
    "A trick that we often use is to translate the high-dimensional sparse feature vectors to low-dimensional dense vectors. These dense vectors are called embeddings. Because the embeddings are low-dimensional, our model needs to learn far fewer weights. Of course, the model does need to learn the embeddings themselves, but this is a trade-off that does pay off. One of the interesting properties of embeddings is that the embedding for <font color='red' face='monospace'>'a'</font> and <font color='red' face='monospace'>'A'</font> are very similar, which means that the rest our network can focus on learning more abstract relations between characters.\n",
    "\n",
    "Another point of view is that learning embeddings is kind of like having an automated pre-processing step included in the model. Pre-processing in such an end-to-end setting ensures optimal performance in the task that we're actually interested in.\n",
    "\n",
    "An embedding matrix in Tensorflow must have the shape `(n_chars, emd_dim)`, where `n_chars` is the number of characters (or tokens) and `emb_dim` is the dimensionality of the dense embedding vector space. We typically initialize the embedding matrix randomly, e.g.\n",
    "```python\n",
    "n_chars = 129\n",
    "emb_dim = 10\n",
    "emb = tf.Variable(tf.random_uniform([n_chars, emb_dim]))\n",
    "```\n",
    "Then, in order to get the relevant embeddings we could use the one-hot encoded (sparse) representation `x_one_hot` (see above) as a mask:\n",
    "```python\n",
    "x_dense = tf.matmul(x_one_hot, emb)\n",
    "```\n",
    "There's a **more efficient** way of doing this, though. For this we use Tensorflow's embedding lookup function:\n",
    "```python\n",
    "x_dense = tf.nn.embedding_lookup(emb, x_indices)\n",
    "```\n",
    "The reason why this is more efficient is that avoid constructing `x_one_hot` explicitly (`x_indices` is enough).\n",
    "\n",
    "In the training process, our model will learn an appropriate embedding matrix `emb` alongside the rest of the model parameters.\n",
    "\n",
    "Below, we show a visual representation of the **character embeddings** as well as the mini-batched dense **input tensor**.\n",
    "\n",
    "![](img/char_embeddings.svg)\n",
    "![](img/rank3_input.svg)\n",
    "\n",
    "We have supplied a simple encoder in the `utils` module, which implements the procedure explained above (plus some more):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bye bye now.', 'Hi again!']\n",
      "[[ 67 122 102  33  99 122 102  33 111 112 120  47   0]\n",
      " [ 73 106  33  98 104  98 106 111  34   0   0   0   0]]\n",
      "\n",
      "['Hello, world!', 'Bye bye now.']\n",
      "[[ 73 102 109 109 112  45  33 120 112 115 109 101  34   0]\n",
      " [ 67 122 102  33  99 122 102  33 111 112 120  47   0   0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import SentenceEncoder\n",
    "\n",
    "sents = [\"Hello, world!\", \"Hi again!\", \"Bye bye now.\"]\n",
    "encoder = SentenceEncoder(sents, batch_size=2)\n",
    "\n",
    "\n",
    "for batch in encoder:\n",
    "    seq = batch[0]\n",
    "    print encoder.decode(seq)\n",
    "    print seq\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>Exercise</font>\n",
    "In this exercise we're going to the functions that we just learned about to translate text into numeric input tensors.\n",
    "\n",
    "#### <font color='green'>A)</font> A simple character encoder.\n",
    "\n",
    "Using the examples above, write a simple encoder that takes the sentences\n",
    "```python\n",
    "sents = ['Hello, world!', 'Bye bye.']\n",
    "```\n",
    "and returns both the encoded sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input sentences\n",
    "sents = ['Hello, world!', 'Bye bye.']\n",
    "\n",
    "# this is the expected output\n",
    "out = [[ 73, 102, 109, 109, 112,  45,  33, 120, 112, 115, 109, 101,  34,   0],\n",
    "       [ 67, 122, 102,  33,  99, 122, 102,  47,   0,   0,   0,   0,   0,   0]]\n",
    "\n",
    "\n",
    "def encode(sents):\n",
    "    '<your code here>'\n",
    "\n",
    "\n",
    "print encode(sents)\n",
    "np.testing.assert_array_equal(out, encode(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load sol/ex_char_encoder.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='green'>B)</font> Get sparse representation.\n",
    "\n",
    "Create a one-hot encoded (sparse) representation of the sentences that we encoded above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear any previous computation graphs\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# dimensions\n",
    "n_chars = '<your code here>'\n",
    "batch_size = '<your code here>'\n",
    "max_seqlen = '<your code here>'\n",
    "\n",
    "# input placeholder\n",
    "sents_enc = '<your code here>'\n",
    "\n",
    "# sparse representation\n",
    "x_one_hot = '<your code here>'\n",
    "\n",
    "# input\n",
    "sents = ['Hello, world!', 'Bye bye.']\n",
    "\n",
    "\n",
    "with tf.Session() as s:\n",
    "    '<your code here>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load sol/ex_one_hot.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='green'>C)</font> Get dense representation.\n",
    "\n",
    "Same as the previous exercise, except now use an embedding matrix to create a **dense** representation of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear any previous computation graphs\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# dimensions\n",
    "n_chars = '<your code here>'\n",
    "batch_size = '<your code here>'\n",
    "emb_dim = '<your code here>'\n",
    "max_seqlen = '<your code here>'\n",
    "\n",
    "# input placeholder\n",
    "sents_enc = '<your code here>'\n",
    "\n",
    "# character embeddings\n",
    "emb = '<your code here>'\n",
    "\n",
    "# dense representation\n",
    "x_dense = '<your code here>'\n",
    "\n",
    "# input\n",
    "sents = ['Hello, world!', 'Bye bye.']\n",
    "\n",
    "\n",
    "with tf.Session() as s:\n",
    "    '<your code here>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load sol/ex_embedding_lookup.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
