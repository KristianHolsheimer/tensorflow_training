{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the next character in a string using LSTMs\n",
    "\n",
    "In this notebook we're going to create a recurrent neural net (RNN) for predicting the next character in a string of characters (i.e. text). This RNN will be able generate full texts, given only a few 'seed' characters to warm up, i.e.\n",
    "\n",
    "```python\n",
    "In [1]: seed = \"Hel\"\n",
    "In [2]: model.predict(seed)\n",
    "Out[2]: 'Hello, world!'\n",
    "```\n",
    "\n",
    "We split up our approach into the following tree easy-to-follow parts:\n",
    "\n",
    "### **[<font color='blue'>Part 1</font>](#Part-1:-Control-flow-in-Tensorflow): Control flow in Tensorflow.**\n",
    "### **[<font color='blue'>Part 2</font>](#Part-2:-Formatting-the-input-data): Formatting the input data.**\n",
    "### **[<font color='blue'>Part 3</font>](#Part-3:-Build-the-RNN): Build the RNN.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>**Part 1**</font>: Control flow in Tensorflow\n",
    "\n",
    "In this notebook, we're going to use control flow operations, i.e. *if* statements and *while* loops. So before we tackle the bigger problem, let's learn how these control flow operations are implemented in Tensorflow.\n",
    "\n",
    "## `if` statements in Tensorflow: `tf.cond`\n",
    "Let's start Tensorflow's 'if' statement: **`tf.cond`** ('cond' stands for 'conditional execution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear previous computation graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# some dummy data\n",
    "x = tf.constant(7)\n",
    "y = tf.constant(13)\n",
    "\n",
    "\n",
    "max_xy = tf.cond(x >= y, lambda: x, lambda: y)\n",
    "\n",
    "\n",
    "with tf.Session() as s:\n",
    "    print s.run(max_xy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note:*** *Also have a look at* **`tf.case`** *for more general switch statements.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>Exercise</font>\n",
    "Write a function using **`tf.cond`** that takes a scalar and returns its modulus (without using **`tf.abs`**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# clear previous computation graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# some test cases\n",
    "x = tf.constant(...)\n",
    "y = tf.constant(...)\n",
    "\n",
    "\n",
    "def my_abs(x):\n",
    "    '<your code here>'\n",
    "\n",
    "\n",
    "with tf.Session() as s:\n",
    "    '<your code here>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load sol/ex_cond_abs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traversing sequences in Tensorflow: `tf.map_fn` and `tf.while_loop`\n",
    "\n",
    "Let's consider an array $x$, e.g.\n",
    "```python\n",
    "x = [0, 1, 2, 3]\n",
    "```\n",
    "There are essentially two distinct ways of traversing an array.\n",
    "\n",
    "#### Type 1: map operations\n",
    "The input for each iteration $i$ depends only on the component $x_i$. This is often called a **map** operation, which are trivially parallelizable (think of the map-reduce paradigm). In Tensorflow, this is implemented in **`tf.map_fn`**.\n",
    "\n",
    "#### Type 2: loops\n",
    "Here, the input for each consecutive iteration $i$ depends on the values computed in the previous iteration $i-1$. In other words, there is a definite notion of order of the sequence of operations. This kind of operation is implemented in Tensorflow in **`tf.while_loop`**.\n",
    "\n",
    "In the following two exercises we're going to learn how to use these two operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/map_fn.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>Exercise</font>\n",
    "#### Use `tf.map_fn` to broadcasting unequal-sized tensors.\n",
    "\n",
    "Now suppose we have the following two tensors:\n",
    "```python\n",
    "a = tf.ones([2, 3, 5])\n",
    "b = tf.ones([7, 5])\n",
    "```\n",
    "and we would like compute the point-wise difference along the last axes (of length 5) in such a way that we get a rank-4 tensor of shape `[7, 2, 3, 5]`. In terms of components, this would be:\n",
    "```python\n",
    "c[i,j,k,l] = a[j,k,l] - b[i,l]\n",
    "```\n",
    "Now, the problem arises when we naively try:\n",
    "```python\n",
    "c = a - b\n",
    "```\n",
    "which raises an error saying:\n",
    "```\n",
    "Dimensions must be equal, but are 3 and 7 for 'sub' (op: 'Sub') with input shapes: [2,3,5], [7,5].\n",
    "```\n",
    "Now we do know that the broadcast works fine if we have a rank-1 tensor (vector) instead of a rank-2 tensor (matrix). In other words, if we select only one row of `b`, the broadcasting will work just fine, i.e.\n",
    "```python\n",
    "a - b[0]  # this works!\n",
    "```\n",
    "Using this information, come up with a way to calculate $c$ with **`tf.map_fn`**.\n",
    "\n",
    "***Note:*** *The output tensor* `c` *must have shape* `(7, 2, 3, 5)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "a = tf.ones([2, 3, 5])\n",
    "b = tf.ones([7, 5])\n",
    "\n",
    "    \n",
    "c = '<your code here>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load sol/ex_map_fn_broadcasting.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/while_loop.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>Exercise</font>\n",
    "\n",
    "In this exercise, we're going to use **`tf.while_loop`** to compute the sum and the cumulative sum of an array of numbers:\n",
    "```python\n",
    "x = tf.constant([7, 0, 42, 1, 13, 4])\n",
    "```\n",
    "\n",
    "#### <font color='green'>A)</font> A simple while loop: *sum*.\n",
    "Write a while loop that computes the sum of the entries of `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# input\n",
    "x_len = 6\n",
    "x = tf.constant([7, 0, 42, 1, 13, 4])\n",
    "\n",
    "\n",
    "def cond(i, acc):\n",
    "    '<your code here>'\n",
    "\n",
    "\n",
    "def body(i, acc):\n",
    "    '<your code here>'\n",
    "\n",
    "\n",
    "# initial values for the loop variables\n",
    "loop_vars = '<your code here>'\n",
    "\n",
    "# compute loop\n",
    "'<your code here>'\n",
    "\n",
    "\n",
    "with tf.Session() as s:\n",
    "    '<your code here>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load sol/ex_while_loop_sum.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the simple stuff out of the way, we're ready to tackle the slightly more involved case in which the accumulator isn't a scalar.\n",
    "\n",
    "#### <font color='green'>B)</font> A slightly trickier while loop: *cumsum*.\n",
    "In this part of the exercise, we'll build on our previous solution to calculate the cumulative sum ox `x`.\n",
    "\n",
    "The main idea is the following. You start with an empty vector:\n",
    "```python\n",
    "y = tf.constant([], dtype=...)\n",
    "```\n",
    "Then, as you're computing the sum the way you did in part <font color='green'>**A**</font>, you keep appending each new entry to `y` with every iteration. To do the 'appending', you could use e.g. **`tf.concat`** in the following way:\n",
    "```python\n",
    "a = tf.constant([7, 1, 13])  # vector\n",
    "b = tf.constant(11)          # scalar\n",
    "\n",
    "# append scalar b to vector a\n",
    "a = tf.concat([a, tf.expand_dims(b, axis=0)], axis=0)\n",
    "\n",
    "```\n",
    "\n",
    "Using these instructions, calculate the cumulative sum using **`tf.while_loop`**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# input\n",
    "x_len = 6\n",
    "x = tf.constant([7, 0, 42, 1, 13, 4])\n",
    "\n",
    "\n",
    "def cond(i, acc, y):\n",
    "    '<your code here>'\n",
    "\n",
    "\n",
    "def body(i, acc, y):\n",
    "    '<your code here>'\n",
    "\n",
    "\n",
    "# initial values for the loop variables\n",
    "loop_vars = '<your code here>'\n",
    "\n",
    "# specify dynamic shape invariant for y\n",
    "shape_invariants = '<your code here>'\n",
    "\n",
    "# compute the loop\n",
    "'<your code here>'\n",
    "\n",
    "\n",
    "with tf.Session() as s:\n",
    "    '<your code here>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load sol/ex_while_loop_cumsum.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>**Part 2**</font>: Formatting the input data\n",
    "\n",
    "Before we can start training we need to prepare our input data in a way that our model will understand it.\n",
    "\n",
    "Since we're dealing with text, we need to turn the characters into numbers in order to perform our calculations on them. We do this in two steps: first we get the sparse (one-hot encoded) representation of each character and then we learn a dense representation (so-called embeddings) as part of our model training.\n",
    "\n",
    "#### Sparse representation: *one-hot encoding*\n",
    "Our sparse representation will consist of sparse vectors of dimension `n_chars`, which in our case is 129 (128 ascii chars + 1 end-of-sequence char). The feature vector for a single character will thus be of the form:\n",
    "\n",
    "$\\qquad x(\\text{char})\\ =\\ (0, 0, 1, 0, \\dots, 0)$\n",
    "\n",
    "Or equivalently in components,\n",
    "\n",
    "$\\qquad x_i(\\text{char})\\ =\\ \\left\\{\\begin{matrix}1&\\text{if } i = h(\\text{char})\\\\0&\\text{otherwise}\\end{matrix}\\right.$\n",
    "\n",
    "where $h$ is a function that maps a character to an integer (e.g. a hash function). In our case, we use the build-in function [**`ord`**](https://docs.python.org/2/library/functions.html#ord):\n",
    "```python\n",
    "In [1]: ord('H')\n",
    "Out[1]: 72\n",
    "```\n",
    "As it turns out, we don't actually need to construct the vector $x(\\text{char})$ as displayed above. If you think about it, the only information that we need about $x$ is which component is switched on. In other words, the only information we need is $h(\\text{char})$, in our case <font face='monospace'><font color='green'>ord</font>(char)</font>. So, the most efficient representation for our sparse feature vectors (single integers) turns out to be incredibly simple. For instance, the sparse representation of the phrase *\"Hello, world!\"* is simply:\n",
    "```python\n",
    "In [1]: x = [ord(char) for char in \"Hello, world!\"]\n",
    "In [2]: x\n",
    "Out[2]: [72, 101, 108, 108, 111, 44, 32, 119, 111, 114, 108, 100, 33]\n",
    "```\n",
    "Actually, we need to append an end-of-sequence (EOS) character to tell our model to stop generating more text. Let's set the index 0 aside for the EOS character, then we one-hot encode our phrase as follows:\n",
    "```python\n",
    "In [1]: x = [ord(char) + 1 for char in \"Hello, world!\"] + [0]\n",
    "In [2]: x\n",
    "Out[2]: [73, 102, 109, 109, 112, 45, 33, 120, 112, 115, 109, 101, 34, 0]\n",
    "```\n",
    "To go from a list of indices to a one-hot encoded vector in Tensorflow is super easy using **`tf.one_hot`**:\n",
    "```python\n",
    "n_chars = 129\n",
    "x_indices = tf.constant([73, 102, 109, 109, 112])\n",
    "x_one_hot = tf.one_hot(x_indices, n_chars)  # shape = (5, 129)\n",
    "\n",
    "```\n",
    "\n",
    "#### Dense representation: *embeddings*\n",
    "If we only have a few input characters, we can use the one-hot encoded representation directly as our input. In reality, though, we know that text consists of a large number characters (in our case 129). In this case it's either infeasible or at best highly inefficient to use the sparse representation for our characters.\n",
    "\n",
    "Moreover, the sparse representation has no notion of proximity between characters such as <font color='red' face='monospace'>'a'</font> and <font color='red' face='monospace'>'A'</font> or more subtly <font color='red' face='monospace'>'i'</font> and <font color='red' face='monospace'>'y'</font>.\n",
    "\n",
    "A trick that we often use is to translate the high-dimensional sparse feature vectors to low-dimensional dense vectors. These dense vectors are called embeddings. Because the embeddings are low-dimensional, our model needs to learn far fewer weights. Of course, the model does need to learn the embeddings themselves, but this is a trade-off that does pay off. One of the interesting properties of embeddings is that the embedding for <font color='red' face='monospace'>'a'</font> and <font color='red' face='monospace'>'A'</font> are very similar, which means that the rest our network can focus on learning more abstract relations between characters.\n",
    "\n",
    "Another point of view is that learning embeddings is kind of like having an automated pre-processing step included in the model. Pre-processing in such an end-to-end setting ensures optimal performance in the task that we're actually interested in.\n",
    "\n",
    "An embedding matrix in Tensorflow must have the shape `(n_chars, emd_dim)`, where `n_chars` is the number of characters (or tokens) and `emb_dim` is the dimensionality of the dense embedding vector space. We typically initialize the embedding matrix randomly, e.g.\n",
    "```python\n",
    "n_chars = 129\n",
    "emb_dim = 10\n",
    "emb = tf.Variable(tf.random_uniform([n_chars, emb_dim]))\n",
    "```\n",
    "Then, in order to get the relevant embeddings we could use the one-hot encoded (sparse) representation `x_one_hot` (see above) as a mask:\n",
    "```python\n",
    "x_dense = tf.matmul(x_one_hot, emb)\n",
    "```\n",
    "There's a **more efficient** way of doing this, though. For this we use Tensorflow's embedding lookup function:\n",
    "```python\n",
    "x_dense = tf.nn.embedding_lookup(emb, x_indices)\n",
    "```\n",
    "The reason why this is more efficient is that avoid constructing `x_one_hot` explicitly (`x_indices` is enough).\n",
    "\n",
    "In the training process, our model will learn an appropriate embedding matrix `emb` alongside the rest of the model parameters.\n",
    "\n",
    "Below, we show a visual representation of the **character embeddings** as well as the mini-batched dense **input tensor**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/char_embeddings.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rank3_input.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module reloaded: utils\n",
      "['Bye bye now.', 'Hi again!']\n",
      "[13 10]\n",
      "[[ 67 122 102  33  99 122 102  33 111 112 120  47   0]\n",
      " [ 73 106  33  98 104  98 106 111  34   0   0   0   0]]\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 0 0 0]]\n",
      "\n",
      "['Hello, world!', 'Bye bye now.']\n",
      "[14 13]\n",
      "[[ 73 102 109 109 112  45  33 120 112 115 109 101  34   0]\n",
      " [ 67 122 102  33  99 122 102  33 111 112 120  47   0   0]]\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload utils\n",
    "from utils import SentenceEncoder\n",
    "\n",
    "sents = [\"Hello, world!\", \"Hi again!\", \"Bye bye now.\"]\n",
    "enc = SentenceEncoder(sents, batch_size=2)\n",
    "\n",
    "\n",
    "for seq, seqlen, seq_mask, max_seqlen in enc:\n",
    "    print enc.decode(seq)\n",
    "    print seqlen\n",
    "    print seq\n",
    "    print seq_mask.astype(int)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>Exercise</font>\n",
    "In this exercise we're going to the functions that we just learned about to translate text into numeric input tensors.\n",
    "\n",
    "#### <font color='green'>A)</font> A simple character encoder.\n",
    "\n",
    "Using the examples above, write a simple encoder that takes the sentences\n",
    "```python\n",
    "sents = ['Hello, world!', 'Bye bye.']\n",
    "```\n",
    "and returns both the encoded sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sol/ex_char_encoder.py\n"
     ]
    }
   ],
   "source": [
    "# input sentences\n",
    "sents = ['Hello, world!', 'Bye bye.']\n",
    "\n",
    "# this is the expected output\n",
    "out = [[ 73, 102, 109, 109, 112,  45,  33, 120, 112, 115, 109, 101,  34,   0],\n",
    "       [ 67, 122, 102,  33,  99, 122, 102,  47,   0,   0,   0,   0,   0,   0]]\n",
    "\n",
    "\n",
    "def encoder(sents):\n",
    "    '<your code here>'\n",
    "\n",
    "\n",
    "print encoder(sents)\n",
    "np.testing.assert_array_equal(out, encoder(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load sol/ex_char_encoder.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='green'>B)</font> Get sparse representation.\n",
    "\n",
    "Create a one-hot encoded (sparse) representation of the sentences that we encoded above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear any previous computation graphs\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# dimensions\n",
    "n_chars = '<your code here>'\n",
    "batch_size = '<your code here>'\n",
    "max_seqlen = '<your code here>'\n",
    "\n",
    "# input placeholder\n",
    "sents_enc = '<your code here>'\n",
    "\n",
    "# sparse representation\n",
    "x_one_hot = '<your code here>'\n",
    "\n",
    "# input\n",
    "sents = ['Hello, world!', 'Bye bye.']\n",
    "\n",
    "\n",
    "with tf.Session() as s:\n",
    "    '<your code here>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load sol/ex_one_hot.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='green'>C)</font> Get dense representation.\n",
    "\n",
    "Same as the previous exercise, except now use an embedding matrix to create a **dense** representation of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear any previous computation graphs\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# dimensions\n",
    "n_chars = '<your code here>'\n",
    "batch_size = '<your code here>'\n",
    "emb_dim = '<your code here>'\n",
    "max_seqlen = '<your code here>'\n",
    "\n",
    "# input placeholder\n",
    "sents_enc = '<your code here>'\n",
    "\n",
    "# character embeddings\n",
    "emb = '<your code here>'\n",
    "\n",
    "# dense representation\n",
    "x_dense = '<your code here>'\n",
    "\n",
    "# input\n",
    "sents = ['Hello, world!', 'Bye bye.']\n",
    "\n",
    "\n",
    "with tf.Session() as s:\n",
    "    '<your code here>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load sol/ex_embedding_lookup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>**Part 3**</font>: Build the RNN\n",
    "\n",
    "Now that we all of the necessary ingredients, it's time to put the pieces together.\n",
    "\n",
    "<font color='red'>**TODO:** Split up into smaller chunks and exercises.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 4.76753139496\n",
      "\n",
      "Seed: \"Hello\"\n",
      "Orig: Hello, world!\n",
      "Pred: }}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}\n",
      "\n",
      "Seed: \"Hi ag\"\n",
      "Orig: Hi again!\n",
      "Pred: }}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 2, loss: 4.49460506439\n",
      "\n",
      "Seed: \"Hello\"\n",
      "Orig: Hello, world!\n",
      "Pred:       oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo\n",
      "\n",
      "Seed: \"Hi ag\"\n",
      "Orig: Hi again!\n",
      "Pred:        ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 4, loss: 3.5093023777\n",
      "\n",
      "Seed: \"Good \"\n",
      "Orig: Good bye now.\n",
      "Pred:   oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo\n",
      "\n",
      "Seed: \"Hello\"\n",
      "Orig: Hello, world!\n",
      "Pred:     oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 8, loss: 2.42736053467\n",
      "\n",
      "Seed: \"Good \"\n",
      "Orig: Good bye now.\n",
      "Pred: H    oooo\n",
      "\n",
      "Seed: \"Hi ag\"\n",
      "Orig: Hi again!\n",
      "Pred: H      oo\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 16, loss: 1.9380351305\n",
      "\n",
      "Seed: \"Hi ag\"\n",
      "Orig: Hi again!\n",
      "Pred: Holllaannn!!\n",
      "\n",
      "Seed: \"Hello\"\n",
      "Orig: Hello, world!\n",
      "Pred: Hollo   ooo\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 32, loss: 0.376168936491\n",
      "\n",
      "Seed: \"Hello\"\n",
      "Orig: Hello, world!\n",
      "Pred: He lo,ywoood\n",
      "\n",
      "Seed: \"Good \"\n",
      "Orig: Good bye now.\n",
      "Pred: Hoo  byeooow\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 64, loss: 0.187294885516\n",
      "\n",
      "Seed: \"Hello\"\n",
      "Orig: Hello, world!\n",
      "Pred: Hello, world!\n",
      "\n",
      "Seed: \"Hi ag\"\n",
      "Orig: Hi again!\n",
      "Pred: Hi a a n\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch: 128, loss: 0.00291905808263\n",
      "\n",
      "Seed: \"Hi ag\"\n",
      "Orig: Hi again!\n",
      "Pred: Hi again!\n",
      "\n",
      "Seed: \"Hello\"\n",
      "Orig: Hello, world!\n",
      "Pred: Hello, world!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWkAAAEOCAYAAABLiuasAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEsb+PzRHR4C9JMzoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fde608eaf90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import SentenceEncoder\n",
    "from itertools import islice\n",
    "\n",
    "# static shapes\n",
    "n_hidden = 64\n",
    "n_chars = 129  # ascii + EOS = 128 + 1 = 129\n",
    "emb_dim = 13\n",
    "batch_size = 2\n",
    "warmup = 5\n",
    "\n",
    "# clear any previous computation graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# character embeddings\n",
    "emb = tf.Variable(tf.random_uniform([n_chars, emb_dim], dtype=tf.float32))\n",
    "\n",
    "# tf Graph input\n",
    "seq_enc = tf.placeholder(tf.int32, [batch_size, None])  # (batch_size, max_seqlen_within_batch)\n",
    "seq_mask = tf.placeholder(tf.bool, [batch_size, None])  # (batch_size, max_seqlen_within_batch)\n",
    "max_seqlen = tf.placeholder(tf.int32, [])\n",
    "\n",
    "# translate to dense vectors\n",
    "x = tf.nn.embedding_lookup(emb, seq_enc)  # (batch_size, max_seqlen_within_batch, emb_dim])\n",
    "x = tf.transpose(x, [1, 0, 2])            # tf.map_fn iterates over axis=0\n",
    "\n",
    "# rnn cell\n",
    "lstm_cell = tf.contrib.rnn.LSTMCell(n_hidden, num_proj=emb_dim, use_peepholes=True)\n",
    "\n",
    "# memory cell states\n",
    "c = tf.Variable(tf.random_uniform([1, n_hidden]))\n",
    "m = tf.Variable(tf.random_uniform([1, emb_dim]))\n",
    "c = tf.tile(c, [batch_size, 1])\n",
    "m = tf.tile(m, [batch_size, 1])\n",
    "\n",
    "\n",
    "def cond(i, h, c, m):\n",
    "    return i < max_seqlen\n",
    "\n",
    "def body(i, h, c, m):\n",
    "    prev_token = tf.cond(\n",
    "        i < warmup,\n",
    "        lambda: x[i],\n",
    "        lambda: tf.squeeze(h[-1])\n",
    "    )\n",
    "    \n",
    "    h_new, (c, m) = lstm_cell(prev_token, (c, m))\n",
    "    h_new = tf.expand_dims(h_new, 0)\n",
    "    \n",
    "    h = tf.concat([h, h_new], axis=0)\n",
    "    i += 1\n",
    "    return i, h, c, m\n",
    "\n",
    "\n",
    "shape_invariants = map(tf.TensorShape, (\n",
    "    [],                           # i.shape\n",
    "    [None, batch_size, emb_dim],  # h.shape\n",
    "    [batch_size, n_hidden],       # c.shape\n",
    "    [batch_size, emb_dim],        # m.shape\n",
    "))\n",
    "\n",
    "# run while loop\n",
    "h = tf.zeros([0, batch_size, emb_dim])\n",
    "h = tf.while_loop(cond, body, [0, h, c, m], shape_invariants)[1]\n",
    "\n",
    "# use Gaussian kernel for inter-embedding distances\n",
    "d = tf.norm(tf.map_fn(lambda e: h - e, emb), axis=-1)  # shape = (n_chars, max_seqlen_within_batch, batch_size)\n",
    "d = tf.transpose(d, [2, 1, 0])                         # shape = (batch_size, max_seqlen_within_batch, n_chars)\n",
    "\n",
    "# define loss function\n",
    "logits = -d ** 2                                         # shape = (batch_size, max_seqlen_within_batch, n_chars)\n",
    "labels = tf.one_hot(seq_enc, n_chars, dtype=tf.float32)  # shape = (batch_size, max_seqlen_within_batch, n_chars)\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "loss = tf.reduce_mean(tf.boolean_mask(loss, seq_mask))   # mask discards loss due to zero-padding\n",
    "\n",
    "# get the most likely predicted characters\n",
    "y_hat = tf.argmax(logits, axis=-1)  # shape = (batch_size, max_seqlen_within_batch)\n",
    "\n",
    "\n",
    "   \n",
    "def fit_model(sents, learning_rate=1e-2, n_epochs=128):\n",
    "    \"\"\"\n",
    "    This function contains the usual boiler plate required to feed the\n",
    "    input data and to print some training diagnostics.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    sents : sequence of strings\n",
    "        The text on which to train / predict.\n",
    "        \n",
    "    learning_rate : float\n",
    "        AdamOptimizer learning rate.\n",
    "        \n",
    "    n_epochs : int\n",
    "        Numer of epochs to use at training time.\n",
    "    \n",
    "    \"\"\"\n",
    "    # optimizer\n",
    "    train = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    with tf.Session() as s:\n",
    "        s.run(tf.global_variables_initializer())\n",
    "        learning_curve = {}\n",
    "        for epoch in xrange(1, n_epochs + 1):\n",
    "            for seq_enc_, seqlen_, seq_mask_, max_seqlen_ in enc:\n",
    "                _, loss_ = s.run(\n",
    "                    (train, loss),\n",
    "                    feed_dict={\n",
    "                        seq_enc: seq_enc_,\n",
    "                        seq_mask: seq_mask_,\n",
    "                        max_seqlen: max_seqlen_})\n",
    "\n",
    "            if np.log2(epoch).is_integer():\n",
    "                print \"Epoch: {}, loss: {}\\n\".format(epoch, loss_)\n",
    "                learning_curve[epoch] = loss_\n",
    "\n",
    "                # create prediction data\n",
    "                seq_enc_ = list(enc)[0][0][:2]  # get just two sentences\n",
    "                seqlen_ = 100\n",
    "                seq_mask_ = np.zeros([batch_size, seqlen_], dtype=np.bool)\n",
    "                seed = np.zeros([batch_size, seqlen_], dtype=np.int32)\n",
    "                seed[:,:warmup] = seq_enc_[:,:warmup]\n",
    "                y_hat_ = s.run(\n",
    "                    y_hat,\n",
    "                    feed_dict={\n",
    "                        seq_enc: seed,\n",
    "                        seq_mask: seq_mask_,\n",
    "                        max_seqlen: seqlen_})\n",
    "\n",
    "                for s1, s2 in zip(enc.decode(seq_enc_), enc.decode(y_hat_)):\n",
    "                    print \"Seed: \\\"{}\\\"\".format(s1[:warmup])\n",
    "                    print \"Orig: {}\".format(s1)\n",
    "                    print \"Pred: {}\\n\".format(s2)\n",
    "\n",
    "                print \"-\" * 80\n",
    "\n",
    "    learning_curve = pd.Series(learning_curve)\n",
    "    learning_curve.plot(logx=True, style='o-', title='KL divergence')\n",
    "\n",
    "\n",
    "# some simple input sentences\n",
    "sents = [\"Hello, world!\", \"Hi again!\", \"Good bye now.\"]\n",
    "\n",
    "fit_model(sents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
