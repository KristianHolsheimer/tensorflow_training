{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Control flow in Tensorflow</font>\n",
    "\n",
    "In this tutorial, we're going to use control flow operations, i.e. *if* statements and *while* loops. So before we tackle the bigger problem, let's learn how these control flow operations are implemented in Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `if` statements in Tensorflow: `tf.cond`\n",
    "Let's start Tensorflow's 'if' statement: **`tf.cond`** ('cond' stands for 'conditional execution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear previous computation graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# some dummy data\n",
    "x = tf.constant(7)\n",
    "y = tf.constant(13)\n",
    "\n",
    "\n",
    "max_xy = tf.cond(x >= y, lambda: x, lambda: y)\n",
    "\n",
    "\n",
    "with tf.Session() as s:\n",
    "    print s.run(max_xy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note:*** *Also have a look at* **`tf.case`** *for more general switch statements.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>Exercise</font>\n",
    "Write a function using **`tf.cond`** that takes a scalar and returns its modulus (without using **`tf.abs`**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# clear previous computation graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# some test cases\n",
    "x = tf.constant(...)\n",
    "y = tf.constant(...)\n",
    "\n",
    "\n",
    "def my_abs(x):\n",
    "    '<your code here>'\n",
    "\n",
    "\n",
    "with tf.Session() as s:\n",
    "    '<your code here>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load sol/ex_cond_abs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traversing sequences in Tensorflow: `tf.map_fn` and `tf.while_loop`\n",
    "\n",
    "Let's consider an array $x$, e.g.\n",
    "```python\n",
    "x = [0, 1, 2, 3]\n",
    "```\n",
    "There are essentially two distinct ways of traversing an array.\n",
    "\n",
    "#### Type 1: map operations\n",
    "The input for each iteration $i$ depends only on the component $x_i$. This is often called a **map** operation, which are trivially parallelizable (think of the map-reduce paradigm). In Tensorflow, this is implemented in **`tf.map_fn`**.\n",
    "\n",
    "#### Type 2: loops\n",
    "Here, the input for each consecutive iteration $i$ depends on the values computed in the previous iteration $i-1$. In other words, there is a definite notion of order of the sequence of operations. This kind of operation is implemented in Tensorflow in **`tf.while_loop`**.\n",
    "\n",
    "In the following two exercises we're going to learn how to use these two operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/map_fn.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>Exercise</font>\n",
    "#### Use `tf.map_fn` to broadcasting unequal-sized tensors.\n",
    "\n",
    "Now suppose we have the following two tensors:\n",
    "```python\n",
    "a = tf.ones([2, 3, 5])\n",
    "b = tf.ones([7, 5])\n",
    "```\n",
    "and we would like compute the point-wise difference along the last axes (of length 5) in such a way that we get a rank-4 tensor of shape `[7, 2, 3, 5]`. In terms of components, this would be:\n",
    "```python\n",
    "c[i,j,k,l] = a[j,k,l] - b[i,l]\n",
    "```\n",
    "Now, the problem arises when we naively try:\n",
    "```python\n",
    "c = a - b\n",
    "```\n",
    "which raises an error saying:\n",
    "```\n",
    "Dimensions must be equal, but are 3 and 7 for 'sub' (op: 'Sub') with input shapes: [2,3,5], [7,5].\n",
    "```\n",
    "Now we do know that the broadcast works fine if we have a rank-1 tensor (vector) instead of a rank-2 tensor (matrix). In other words, if we select only one row of `b`, the broadcasting will work just fine, i.e.\n",
    "```python\n",
    "a - b[0]  # this works!\n",
    "```\n",
    "Using this information, come up with a way to calculate $c$ with **`tf.map_fn`**.\n",
    "\n",
    "***Note:*** *The output tensor* `c` *must have shape* `(7, 2, 3, 5)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "a = tf.ones([2, 3, 5])\n",
    "b = tf.ones([7, 5])\n",
    "\n",
    "    \n",
    "c = '<your code here>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load sol/ex_map_fn_broadcasting.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/while_loop.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>Exercise</font>\n",
    "\n",
    "In this exercise, we're going to use **`tf.while_loop`** to compute the sum and the cumulative sum of an array of numbers:\n",
    "```python\n",
    "x = tf.constant([7, 0, 42, 1, 13, 4])\n",
    "```\n",
    "\n",
    "#### <font color='green'>A)</font> A simple while loop: *sum*.\n",
    "Write a while loop that computes the sum of the entries of `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# input\n",
    "x_len = 6\n",
    "x = tf.constant([7, 0, 42, 1, 13, 4])\n",
    "\n",
    "\n",
    "def cond(i, acc):\n",
    "    '<your code here>'\n",
    "\n",
    "\n",
    "def body(i, acc):\n",
    "    '<your code here>'\n",
    "\n",
    "\n",
    "# initial values for the loop variables\n",
    "loop_vars = '<your code here>'\n",
    "\n",
    "# compute loop\n",
    "'<your code here>'\n",
    "\n",
    "\n",
    "with tf.Session() as s:\n",
    "    '<your code here>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load sol/ex_while_loop_sum.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the simple stuff out of the way, we're ready to tackle the slightly more involved case in which the accumulator isn't a scalar.\n",
    "\n",
    "#### <font color='green'>B)</font> A slightly trickier while loop: *cumsum*.\n",
    "In this part of the exercise, we'll build on our previous solution to calculate the cumulative sum ox `x`.\n",
    "\n",
    "The main idea is the following. You start with an empty vector:\n",
    "```python\n",
    "y = tf.constant([], dtype=...)\n",
    "```\n",
    "Then, as you're computing the sum the way you did in part <font color='green'>**A**</font>, you keep appending each new entry to `y` with every iteration. To do the 'appending', you could use e.g. **`tf.concat`** in the following way:\n",
    "```python\n",
    "a = tf.constant([7, 1, 13])  # vector\n",
    "b = tf.constant(11)          # scalar\n",
    "\n",
    "# append scalar b to vector a\n",
    "a = tf.concat([a, tf.expand_dims(b, axis=0)], axis=0)\n",
    "\n",
    "```\n",
    "\n",
    "Using these instructions, calculate the cumulative sum using **`tf.while_loop`**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# input\n",
    "x_len = 6\n",
    "x = tf.constant([7, 0, 42, 1, 13, 4])\n",
    "\n",
    "\n",
    "def cond(i, acc, y):\n",
    "    '<your code here>'\n",
    "\n",
    "\n",
    "def body(i, acc, y):\n",
    "    '<your code here>'\n",
    "\n",
    "\n",
    "# initial values for the loop variables\n",
    "loop_vars = '<your code here>'\n",
    "\n",
    "# specify dynamic shape invariant for y\n",
    "shape_invariants = '<your code here>'\n",
    "\n",
    "# compute the loop\n",
    "'<your code here>'\n",
    "\n",
    "\n",
    "with tf.Session() as s:\n",
    "    '<your code here>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load sol/ex_while_loop_cumsum.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
