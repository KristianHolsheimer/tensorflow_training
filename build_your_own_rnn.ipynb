{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Build your own RNN</font>\n",
    "\n",
    "Now that we all of the necessary ingredients, it's time to put the pieces together.\n",
    "\n",
    "<font color='red'>**TODO:** Split up into smaller chunks and exercises.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Convenient, comfortable, safe',\n",
       " '- very nice decorated and clean rooms with AC  - great breakfast (breakfast area is very small and you might have to wait if it is full - so plan enough time for breakfast it is worth it)  - very friendly host  - great location within walking distance to many restaurants, chinese fishing nets, ferry, ...',\n",
       " '- accommodation is not easy to find as it is not directly in K B Jacob Road, but in a very small street parallel to it (close to a church)',\n",
       " 'Good location.Clean room and bath perhaps needing a bit of renovation.Receptionist was very nice and helpful.Packed a take away breakfast for us early next morning.She was great.',\n",
       " 'The room, the wifi and the breakfast',\n",
       " 'There was no AC in August.',\n",
       " 'Feel great with its rooftop pool.   Friendly staff although some english may not so fluent but they are try to serve well to us.   Location is some distance from pub street but still a good location which within town area.',\n",
       " 'Fantastic fitness and spa area, fitness room open for 24 hours, nice pool, the most comfortable bed Ive ever slept in and very nice breakfast-first time ever that I got a selection if smoothies for my breakfast',\n",
       " 'Everything would be perfect here if it werent for the shower.The bathroom per se is quite nice, but the shower is a rather old bath tub with some moldy spots on the wall in one corner. And there was no possibility to plug in the hair straightener in the bathroom, but really mibir issues compared to the whole experience',\n",
       " 'The location has a Keisei Line to get to Narita.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "from itertools import islice\n",
    "from utils import SentenceEncoder\n",
    "\n",
    "\n",
    "def reviews_iter(filepath_pattern):\n",
    "    for filepath in glob(filepath_pattern):\n",
    "        with open(filepath) as f:\n",
    "            for line in f:\n",
    "                line = line.strip().rsplit('\\t', 2)  # review_text, y, review_id\n",
    "                if len(line) == 3 and len(line[0]) > 20:                  \n",
    "                    yield line[0]\n",
    "                \n",
    "\n",
    "sents = list(islice(reviews_iter(\"/home/kris/Data/hotel_reviews_positive_negative.tsv/part-*\"), 128 * 1024))\n",
    "sents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module reloaded: utils\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-dc6964ddac31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;31m# sents = [\"Hello, world!\", \"Hi again!\", \"Good bye now.\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-49-dc6964ddac31>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(sents, learning_rate, n_epochs)\u001b[0m\n\u001b[1;32m    114\u001b[0m                         \u001b[0mseq_enc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq_enc_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                         \u001b[0mseq_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq_mask_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                         max_seqlen: max_seqlen_})\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 896\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    897\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1279\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1280\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1283\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1262\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1263\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%reload utils\n",
    "from utils import SentenceEncoder\n",
    "from itertools import islice, izip\n",
    "\n",
    "\n",
    "# static shapes\n",
    "n_hidden = 64\n",
    "n_chars = 257  # |unicode| + EOS = 256 + 1 = 257\n",
    "emb_dim = 13\n",
    "batch_size = 128\n",
    "warmup = 20\n",
    "\n",
    "# clear any previous computation graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# character embeddings\n",
    "emb = tf.Variable(tf.random_uniform([n_chars, emb_dim], dtype=tf.float32))\n",
    "\n",
    "# tf Graph input\n",
    "seq_enc = tf.placeholder(tf.int32, [batch_size, None])  # shape: (batch_size, max_seqlen)\n",
    "seq_mask = tf.placeholder(tf.bool, [batch_size, None])  # shape: (batch_size, max_seqlen)\n",
    "max_seqlen = tf.placeholder(tf.int32, [])               # max_seqlen varies with each batch\n",
    "\n",
    "# translate to dense vectors\n",
    "x = tf.nn.embedding_lookup(emb, seq_enc)                # shape: (batch_size, max_seqlen, emb_dim])\n",
    "\n",
    "# rnn cell\n",
    "lstm_cell = tf.contrib.rnn.LSTMCell(n_hidden, num_proj=emb_dim, use_peepholes=True)\n",
    "\n",
    "# memory cell states\n",
    "c = tf.Variable(tf.random_uniform([1, n_hidden]))\n",
    "m = tf.Variable(tf.random_uniform([1, emb_dim]))\n",
    "\n",
    "# replicate c and m to allow for batch-wise processing\n",
    "c = tf.tile(c, [batch_size, 1])\n",
    "m = tf.tile(m, [batch_size, 1])\n",
    "\n",
    "\n",
    "def cond(i, h, c, m):\n",
    "    return i < max_seqlen\n",
    "\n",
    "def body(i, h, c, m):\n",
    "    # get token from previous time step\n",
    "    prev_token = tf.cond(\n",
    "        i < warmup,\n",
    "        lambda: x[:, i, :],         # we traverse x in the 'max_seqlen' axis\n",
    "        lambda: tf.squeeze(h[-1]))  # tf.squeeze undoes tf.expand_dims\n",
    "    i += 1\n",
    "\n",
    "    # apply LSTM cell\n",
    "    h_new, (c, m) = lstm_cell(prev_token, (c, m))\n",
    "    \n",
    "    # append h_new to output tensor h\n",
    "    h = tf.concat([h, tf.expand_dims(h_new, 0)], axis=0)\n",
    "\n",
    "    return i, h, c, m\n",
    "\n",
    "\n",
    "shape_invariants = map(tf.TensorShape, (\n",
    "    [],                           # i.shape\n",
    "    [None, batch_size, emb_dim],  # h.shape\n",
    "    [batch_size, n_hidden],       # c.shape\n",
    "    [batch_size, emb_dim],        # m.shape\n",
    "))\n",
    "\n",
    "# run while loop\n",
    "h = tf.zeros([0, batch_size, emb_dim])\n",
    "h = tf.while_loop(cond, body, [0, h, c, m], shape_invariants)[1]\n",
    "\n",
    "# use Euclidean for inter-embedding distances\n",
    "d = tf.norm(tf.map_fn(lambda e: h - e, emb), axis=-1)    # shape: (n_chars, max_seqlen, batch_size)\n",
    "d = tf.transpose(d, [2, 1, 0])                           # shape: (batch_size, max_seqlen, n_chars)\n",
    "\n",
    "# define loss function (Gaussian-kernel KL divergence)\n",
    "logits = -d ** 2                                         # shape: (batch_size, max_seqlen, n_chars)\n",
    "labels = tf.one_hot(seq_enc, n_chars, dtype=tf.float32)  # shape: (batch_size, max_seqlen, n_chars)\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "loss = tf.reduce_mean(tf.boolean_mask(loss, seq_mask))   # mask discards loss due to zero-padding\n",
    "\n",
    "# get the most likely predicted characters\n",
    "y_hat = tf.argmax(logits, axis=-1)                       # shape: (batch_size, max_seqlen)\n",
    "\n",
    "\n",
    "   \n",
    "def fit_model(sents, learning_rate=1e-2, n_epochs=128):\n",
    "    \"\"\"\n",
    "    This function contains the usual boiler plate required to feed the\n",
    "    input data and to print some training diagnostics.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    sents : sequence of strings\n",
    "        The text on which to train / predict.\n",
    "        \n",
    "    learning_rate : float\n",
    "        AdamOptimizer learning rate.\n",
    "        \n",
    "    n_epochs : int\n",
    "        Numer of epochs to use at training time.\n",
    "    \n",
    "    \"\"\"\n",
    "    # optimizer\n",
    "    train = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    encoder = SentenceEncoder(sents, batch_size=batch_size)\n",
    "\n",
    "    with tf.Session() as s:\n",
    "        s.run(tf.global_variables_initializer())\n",
    "        learning_curve = {}\n",
    "        for epoch in xrange(1, n_epochs + 1):\n",
    "            for seq_enc_, seqlen_, seq_mask_, max_seqlen_ in encoder:\n",
    "                _, loss_ = s.run(\n",
    "                    (train, loss),\n",
    "                    feed_dict={\n",
    "                        seq_enc: seq_enc_,\n",
    "                        seq_mask: seq_mask_,\n",
    "                        max_seqlen: max_seqlen_})\n",
    "\n",
    "            if np.log2(epoch).is_integer():\n",
    "                print \"Epoch: {}, loss: {}\\n\".format(epoch, loss_)\n",
    "                learning_curve[epoch] = loss_\n",
    "\n",
    "                # create prediction data\n",
    "                seq_enc_ = list(encoder)[0][0]  # get just two sentences\n",
    "                seqlen_ = 100\n",
    "                seq_mask_ = np.zeros([batch_size, seqlen_], dtype=np.bool)\n",
    "                seed = np.zeros(seq_enc_.shape, dtype=np.int32) \n",
    "                seed[:,:warmup] = seq_enc_[:,:warmup]\n",
    "                y_hat_ = s.run(\n",
    "                    y_hat,\n",
    "                    feed_dict={\n",
    "                        seq_enc: seed,\n",
    "                        seq_mask: seq_mask_,\n",
    "                        max_seqlen: seqlen_})\n",
    "\n",
    "                for s1, s2 in islice(izip(encoder.decode(seq_enc_), encoder.decode(y_hat_)), 3):\n",
    "                    print u\"Seed: \\\"{}\\\"\".format(s1[:warmup])\n",
    "                    print u\"Orig: {}\".format(s1)\n",
    "                    print u\"Pred: {}\\n\".format(s2)\n",
    "\n",
    "                print \"-\" * 80\n",
    "\n",
    "    learning_curve = pd.Series(learning_curve)\n",
    "    learning_curve.plot(logx=True, style='o-', title='KL divergence')\n",
    "\n",
    "\n",
    "# some simple input sentences\n",
    "# sents = [\"Hello, world!\", \"Hi again!\", \"Good bye now.\"]\n",
    "\n",
    "fit_model(sents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
